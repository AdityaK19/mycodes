{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "openCV_codes.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4dNRZYodE8o"
      },
      "source": [
        "TRANSLATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAxss2NLdJDn"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "image = cv2.imread('/content/dave-goudreau-e-Jq0_SQux8-unsplash.jpg')#storing width and height\n",
        "height,width = image.shape[:2]\n",
        "\n",
        "quarter_height, quarter_width = height/4, width/4\n",
        "\n",
        "#translation matrix\n",
        "T = np.float32([[1, 0, quarter_width], [0, 1, quarter_height]])\n",
        "\n",
        "#warpAffine to translate image\n",
        "img_translation = cv2.warpAffine(image, T, (width, height))\n",
        "cv2.imshow('translation', img_translation)\n",
        "\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEcA-1oXdLbD"
      },
      "source": [
        "ROTATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6xVmlhSldNhv"
      },
      "source": [
        "#rotation matrix = getRotationMatrix2d(coordinates, angle, scaling factor)\n",
        "rotation_matrix = cv2.getRotationMatrix2D((width/2, height/2), 90, 1)\n",
        "\n",
        "#warpAffine to rotate image\n",
        "img_rotation = cv2.warpAffine(image, rotation_matrix, (height, width))\n",
        "cv2.imshow(' rotation', img_rotation)\n",
        "\n",
        "#another method to rotate image by 90 degrees\n",
        "img = cv2.transpose(image)\n",
        "cv2.imshow(' rotation', img)\n",
        "\n",
        "cv2.waitKey()\n",
        "cv2.destroyAllWindows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F784RGdedRY_"
      },
      "source": [
        "SCALING, RESIZING, INTERPOLATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsDT-53mdVbM"
      },
      "source": [
        "#cv2.INTER_AREA: This is used when we need to shrink an image.\n",
        "#cv2.INTER_CUBIC: This is slow but more efficient.\n",
        "c#v2.INTER_LINEAR: This is primarily used when zooming is required. This is the default interpolation technique in OpenCV. \n",
        "half = cv2.resize(image, (0, 0), fx = 0.1, fy = 0.1)\n",
        "bigger = cv2.resize(image, (1050, 1610))\n",
        " \n",
        "stretch_near = cv2.resize(image, (780, 540),\n",
        "               interpolation = cv2.INTER_NEAREST)\n",
        " \n",
        " \n",
        "Titles =[\"Original\", \"Half\", \"Bigger\", \"Interpolation Nearest\"]\n",
        "images =[image, half, bigger, stretch_near]\n",
        "count = 4\n",
        " \n",
        "for i in range(count):\n",
        "    plt.subplot(2, 2, i + 1)\n",
        "    plt.title(Titles[i])\n",
        "    plt.imshow(images[i])\n",
        " \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZquEi_wdZ5g"
      },
      "source": [
        "CROPPING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K6qC570dYTR"
      },
      "source": [
        "# Cropping an image\n",
        "cropped_image = img[80:280, 150:330]\n",
        "\n",
        "# Display cropped image\n",
        "cv2.imshow(\"cropped\", cropped_image)\n",
        "\n",
        "# Save the cropped image\n",
        "cv2.imwrite(\"Cropped Image.jpg\", cropped_image)\n",
        "\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CYDDCLIdgC2"
      },
      "source": [
        "BITWISE OPERATIONS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ayTnZY0djOh"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "image = cv2.imread('/content/dave-goudreau-e-Jq0_SQux8-unsplash.jpg')\n",
        "#75 is used to add brightness to our image, more the number, more the brightness\n",
        "M = np.ones(image.shape, dtype = \"uint8\")*75\n",
        "\n",
        "added = cv2.add(M, image)\n",
        "cv2.imshow('added', added)\n",
        "\n",
        "subtracted = cv2.subtract(M, image)\n",
        "cv2.imshow('subtracted', subtracted)\n",
        "\n",
        "cv2.waitKey()\n",
        "cv2.destroyAllWindows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emv_bfMvdlY1"
      },
      "source": [
        "CREATING SOME SIMPLE IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJJx6omKdqly"
      },
      "source": [
        "#making a square\n",
        "square = np.zeros((300, 300), np.uint8)\n",
        "cv2.rectangle(square, (50, 50), (250, 250), 255, -2)\n",
        "cv2.imshow('square', square)\n",
        "cv2.waitkey()\n",
        "\n",
        "#making an ellipse\n",
        "ellipse = np.zeros((300, 300), np.uint8)\n",
        "cv2.ellipse(ellipse, (150, 150), (150, 150), 30, 0, 180, 255, -1)\n",
        "cv2.imshow('ellipse', ellipse)\n",
        "cv2.waitkey()\n",
        "\n",
        "#to show where the intersect\n",
        "And = cv2.bitwise_and(square, ellipse)\n",
        "cv2.imshow('And', And)\n",
        "cv2.waitkey()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G4VF2VcKdrPz"
      },
      "source": [
        "BLURRING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QtMoiKNEdtIR"
      },
      "source": [
        "#creating a 3x3 kernel\n",
        "kernel_3x3 = np.ones((3,3), np.float32)/9\n",
        "\n",
        "#we use cv2.filter2D for blurring out our image\n",
        "blurred1 = cv2.filter2D(image, -1, kernel_3x3)\n",
        "imshow('blurred1', blurred1)\n",
        "cv2.waitkey()\n",
        "\n",
        "#we can also convolve it with 7x7 matrix\n",
        "kernel_7x7 = np.ones((7,7), np.float32)/49\n",
        "blurred2 = cv2.filter2D(image, -1, kernel_7x7)\n",
        "imshow('blurred2', blurred2)\n",
        "cv2.waitkey()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bw_HJ1OdyGY"
      },
      "source": [
        "OTHER BLURRING METHODS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC6DB6vBdz8F"
      },
      "source": [
        "#averaging done with normalised box filter\n",
        "#this takesthe pixels under the box and relaces the central element\n",
        "#box needs to be odd and positive\n",
        "blur = cv2.blur(image, (3, 3))\n",
        "cv2.imshow('Averaging', blur)\n",
        "\n",
        "#insted of box filter, use gaussian kernel\n",
        "gaussian = cv2.GaussianBlur(image, (7,7), 0)\n",
        "cv2.imshow('Gaussian blurring', gaussian)\n",
        "cv2.waitkey(0)\n",
        "\n",
        "#takes median of all pixels of the kernel\n",
        "# replaces central element with median\n",
        "median = cv2.medianBlur(image, 5)\n",
        "cv2.imshow('Median blur', median)\n",
        "cv2.waitkey(0)\n",
        "\n",
        "#bilateral is useful while removing noise and keeping edges sharp\n",
        "bilateral = cv2.bilateralFilter(image, 9, 75, 75)\n",
        "cv2.imshow('bilateral blur', bilateral)\n",
        "cv2.waitkey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLZ2sbgpd3bG"
      },
      "source": [
        "IMAGE DE-NOISING NON LOCAL MEANS DENOISING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU4GqLhHd6dY"
      },
      "source": [
        "dst = cv2.fastNlMeansDenoisingColored(image, None, 6, 6, 7, 21)\n",
        "\n",
        "cv2.imshow('Fast Means Denoising', dst)\n",
        "cv2.waitkey(0)\n",
        "\n",
        "#there are other ways of non local means denoising\n",
        "# cv2.fastNIMeansDenoising()- works with single grayscale images\n",
        "# cv2.fastNiMeansDenoising()- works with colored image\n",
        "# cv2.fastNIMeansDenoisingMulti()- works with image sequence captured in short period of time\n",
        "# cv2.fastNIMeansDenoisingColoredMulti()- same as above but for colored image (above is for grayscale)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZ4l8I6Kd9Qe"
      },
      "source": [
        "SHARPENING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQS6WU_Sd_bY"
      },
      "source": [
        "#create sharpening kernel\n",
        "kernel_sharpening = np.array([[-1, -1, -1], [-1, 9, -1], [-1, -1, -1]])\n",
        "\n",
        "#applying different kernels to sharpened image\n",
        "sharpened = cv2.filter2D(image, -1, kernel_sharpening)\n",
        "\n",
        "cv2.imshow('sharpened', sharpened)\n",
        "cv2.waitkey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_rCDZNHmeBWD"
      },
      "source": [
        "THRESHHOLDING AND BINARIZATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f92IxrgveDj_"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "image = cv2.imread('/content/dave-goudreau-e-Jq0_SQux8-unsplash.jpg')\n",
        "ret,thresh1 = cv2.threshold(image,127,255,cv2.THRESH_BINARY)\n",
        "ret,thresh2 = cv2.threshold(image,127,255,cv2.THRESH_BINARY_INV)\n",
        "ret,thresh3 = cv2.threshold(image,127,255,cv2.THRESH_TRUNC)\n",
        "ret,thresh4 = cv2.threshold(image,127,255,cv2.THRESH_TOZERO)\n",
        "ret,thresh5 = cv2.threshold(image,127,255,cv2.THRESH_TOZERO_INV)\n",
        "cv2.imshow(thresh1)\n",
        "cv2.waitkey(0)\n",
        "cv2.clearAllWIndows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47GFEQyAeGkB"
      },
      "source": [
        "ADAPTIVE THRESHHOLDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZOkvDwBeI2m"
      },
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "img = cv.imread('noisy2.png',0)\n",
        "# global thresholding\n",
        "ret1,th1 = cv.threshold(img,127,255,cv.THRESH_BINARY)\n",
        "# Otsu's thresholding\n",
        "ret2,th2 = cv.threshold(img,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "# Otsu's thresholding after Gaussian filtering\n",
        "blur = cv.GaussianBlur(img,(5,5),0)\n",
        "ret3,th3 = cv.threshold(blur,0,255,cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
        "# plot all the images and their histograms\n",
        "images = [img, 0, th1,\n",
        "          img, 0, th2,\n",
        "          blur, 0, th3]\n",
        "titles = ['Original Noisy Image','Histogram','Global Thresholding (v=127)',\n",
        "          'Original Noisy Image','Histogram',\"Otsu's Thresholding\",\n",
        "          'Gaussian filtered Image','Histogram',\"Otsu's Thresholding\"]\n",
        "for i in xrange(3):\n",
        "    plt.subplot(3,3,i*3+1),plt.imshow(images[i*3],'gray')\n",
        "    plt.title(titles[i*3]), plt.xticks([]), plt.yticks([])\n",
        "    plt.subplot(3,3,i*3+2),plt.hist(images[i*3].ravel(),256)\n",
        "    plt.title(titles[i*3+1]), plt.xticks([]), plt.yticks([])\n",
        "    plt.subplot(3,3,i*3+3),plt.imshow(images[i*3+2],'gray')\n",
        "    plt.title(titles[i*3+2]), plt.xticks([]), plt.yticks([])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3pRr2Uz4eLEF"
      },
      "source": [
        "EROSION AND DILATION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4E_3_YeMeNGm"
      },
      "source": [
        "import cv2  \n",
        "import numpy as np  \n",
        "img = cv2.imread(r'C:\\Users\\DEVANSH SHARMA\\baloon.jpg', 1)  \n",
        "# cv2.erode(src, dst, kernel) cv2.dilate(src, dst, kernel)  \n",
        "\n",
        "kernel = np.ones((5,5), np.uint8)  \n",
        "img_erosion = cv2.erode(img, kernel, iterations=1)  \n",
        "img_dilation = cv2.dilate(img, kernel, iterations=1)  \n",
        "cv2.imshow('Input', img)  \n",
        "cv2.imshow('Erosion', img_erosion)  \n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdOxZ4z5ePSB"
      },
      "source": [
        "EDGE DETECTION AND IMAGE GRADIENTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-0omx8XeRdC"
      },
      "source": [
        "height, width =image.shape\n",
        "\n",
        "#extract sobel edges\n",
        "sobel_x = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\n",
        "sobel_y = cv2.Sobel(image, cv2.CV_64F, 1 , 0, ksize=5)\n",
        "sobel_OR = cv2.bitwise_or(sobel_x, sobel_y)\n",
        "laplacian = cv2.Laplacian(image, cv2.CV_64F)\n",
        "\n",
        "#show everything\n",
        "cv2.imshow('sobel_x', sobel_x)\n",
        "cv2.waitkey(0)\n",
        "cv2.imshow('sobel_y', sobel_y)\n",
        "cv2.waitkey(0)\n",
        "cv2.imshow('sobel_OR', sobel_OR)\n",
        "cv2.waitkey(0)\n",
        "cv2.imshow('laplacian', laplacian)\n",
        "cv2.waitkey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLWg-ZTLeUtZ"
      },
      "source": [
        "GET PERSPECTIVE TRANSFORM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_wrYTFL9eYXz"
      },
      "source": [
        "# coordinates of 4 corners of image\n",
        "points_A = np.float32([320,15], [700,215], [85, 610], [530, 780])\n",
        "\n",
        "#coordinates of desired paper (here a4)\n",
        "points_B = np.float32([0,0], [420,0], [0,594], [594,420])\n",
        "\n",
        "#use the 2 sets of 4 pointe to compute the perspective transformation of matrix, M\n",
        "M = cv2.getPerspectiveTransform(points_A, points_B)\n",
        "\n",
        "warped = cv2.warpPerspective(image, M, (420, 594))\n",
        "\n",
        "cv2.imshow('warPerspective', warped)\n",
        "waitkey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTePuvSpeasL"
      },
      "source": [
        "AFFINE TRANSFORM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f99hy9nwedRQ"
      },
      "source": [
        "# coordinates of 3 corners of image\n",
        "points_A = np.float32([320,15], [700,215], [85, 610])\n",
        "\n",
        "#coordinates of desired paper (here a4)\n",
        "points_B = np.float32([0,0], [420,0], [0,594], [594,420])\n",
        "\n",
        "M = cv2.getAffineTransform(points_A, points_B)\n",
        "\n",
        "warped = cv2.warpAffine(image, M, (420, 594))\n",
        "\n",
        "cv2.imshow('warPerspective', warped)\n",
        "waitkey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X176Q-p8efU5"
      },
      "source": [
        "CONTOURS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4PxFQsLei7U"
      },
      "source": [
        "import numpy as np\n",
        "import cv2 as cv\n",
        "im = cv.imread('test.jpg')\n",
        "imgray = cv.cvtColor(im, cv.COLOR_BGR2GRAY)\n",
        "ret, thresh = cv.threshold(imgray, 127, 255, 0)\n",
        "im2, contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "# draw contours\n",
        "\n",
        "#To draw all the contours in an image:\n",
        "cv.drawContours(img, contours, -1, (0,255,0), 3)\n",
        "\n",
        "#To draw an individual contour, say 4th contour:\n",
        "cv.drawContours(img, contours, 3, (0,255,0), 3)\n",
        "\n",
        "#But most of the time, below method will be useful:\n",
        "cnt = contours[4]\n",
        "cv.drawContours(img, [cnt], 0, (0,255,0), 3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXYni6ZcekvU"
      },
      "source": [
        "SORTING CONTOURS BY AREA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0GQ6aecelRn"
      },
      "source": [
        "import cv2\n",
        "\n",
        "image= cv2.imread('Shapes.png')\n",
        "original_image= image\n",
        "\n",
        "gray= cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "edged= cv2.Canny(gray, 50,200)\n",
        "\n",
        "contours, hierarchy= cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "\n",
        "cv2.imshow('Original Image', image)\n",
        "cv2.waitKey(0)\n",
        "\n",
        "\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "\n",
        "def get_contour_areas(contours):\n",
        "\n",
        "    all_areas= []\n",
        "\n",
        "    for cnt in contours:\n",
        "        area= cv2.contourArea(cnt)\n",
        "        all_areas.append(area)\n",
        "\n",
        "    return all_areas\n",
        "\n",
        "\n",
        "\n",
        "print (\"Contour Areas before Sorting\", get_contour_areas(contours))\n",
        "\n",
        "sorted_contours= sorted(contours, key=cv2.contourArea, reverse= True)\n",
        "\n",
        "print (\"Contour Areas after Sorting\", get_contour_areas(sorted_contours))\n",
        "\n",
        "\n",
        "for c in sorted_contours:\n",
        "    cv2.drawContours(original_image, [c], -1, (255,0,0),10)\n",
        "    cv2.waitKey(0)\n",
        "    cv2.imshow('Contours By Area', original_image)\n",
        "\n",
        "\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pAwV07l6esxA"
      },
      "source": [
        "SORTING CONTOURS BY POSITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcIf2rBbevDK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Gq_VejQevxn"
      },
      "source": [
        "APPROXIMATING CONTOURS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_SVLM3fexse"
      },
      "source": [
        "orig_image = image.copy()\n",
        "\n",
        "#grayscale and binarize\n",
        "gray = cv2.cvtColor(image, cv2.COLOUR_BGR2GRAY)\n",
        "ret, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY_INV)\n",
        "\n",
        "#find contours\n",
        "contours, heirarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "#iterate through each contour and compute the bounding rectangle\n",
        "for c in contours:\n",
        "  #calculate accuracy as percent of contour perimeter\n",
        "  accuracy = 0.03*cv2.arcLength(c, True)\n",
        "  approx = cv2.approxPolyDP(c, accuracy, True)\n",
        "  cv2.drawContours(image, [approx], 0, (0, 255, 0), 2)\n",
        "  cv2.imshow('Approx poly DP', image)\n",
        "\n",
        " cv2.waitkey(0)\n",
        " cv2.destroyAllWindows"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HI4RHHCEe2Ul"
      },
      "source": [
        "CONVEX HULL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hNfYcEFe4Yl"
      },
      "source": [
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#threshold the image\n",
        "ret, thresh = cv2.threshold(gray, 176, 255, 0)\n",
        "\n",
        "#find contours\n",
        "contours, heirarchy = cv2.findContours(thresh.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_NONE)\n",
        "\n",
        "#sort contours by area and then remove the largest frame contour\n",
        "n = len(contours)-1\n",
        "contours = sorted(contours, key = cv2.contourArea, reverse=False)[:n]\n",
        "\n",
        "#iterate over contours and draw a convex hull\n",
        "for c in contours:\n",
        "  hull = cv2.convexHull(c)\n",
        "  cv2.drawContours(image, [hull], 0, (0, 255, 0), 2)\n",
        "  cv2.imshow('convex Hull', image)\n",
        "\n",
        "cv2.waitkey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBjGthbre6Re"
      },
      "source": [
        "MATCHING CONTOUR SHAPES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7HYsL7_e8CU"
      },
      "source": [
        "# load the shape tempelate or reference image\n",
        "tempelate = cv2.imread('images/4star.jpg', 0)\n",
        "cv2.imshow('tempelate', tempelate)\n",
        "cv2.waitkey(0)\n",
        "\n",
        "#load the target image\n",
        "target = cv2.imread('images/star.jpg')\n",
        "target_gray = cv2.cvtColor(target, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "#threshhold both the images\n",
        "ret, thresh1 = cv2.threshold(tempelate, 127, 255, 0)\n",
        "ret, thresh2 = cv2.threshold(tempelate, 127, 255, 0)\n",
        "\n",
        "#finding contours\n",
        "contours, heirarchy = cv2.findContours(thresh1, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "#we need to sort the largest contour which is the shape of outline by sorting by area\n",
        "sorted_contours = sorted(contours, key = cv2.contourArea, reverse=True)\n",
        "\n",
        "#extract 2nd largest contour which will be our template contour\n",
        "template_contour = contours[1]\n",
        "\n",
        "#extract contour from second target image\n",
        "contours, heirarchy = cv2.findContours(thresh2, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
        "\n",
        "for c in contours:\n",
        "  #match the shapes\n",
        "  match = cv2.matchShapes(tempelate_contour, c, 1, 0.0)\n",
        "  print(match)\n",
        "  #if match is less than 0.15\n",
        "  if match<0.15:\n",
        "    closest_contour = c\n",
        "  else:\n",
        "    closest_contour = []\n",
        "\n",
        "cv2.drawContours(target, [closest_contour], -1, (0, 255, 0), 3)\n",
        "cv2.imshow('target', target)\n",
        "cv2.waitkey(0)\n",
        "cvw.clearAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EMfH1Ugoe_vZ"
      },
      "source": [
        "LINE DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ZR676lffBv_"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "  \n",
        "# Reading the required image in \n",
        "# which operations are to be done. \n",
        "# Make sure that the image is in the same \n",
        "# directory in which this python program is\n",
        "img = cv2.imread('image.jpg')\n",
        "  \n",
        "# Convert the img to grayscale\n",
        "gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
        "  \n",
        "# Apply edge detection method on the image\n",
        "edges = cv2.Canny(gray,50,150,apertureSize = 3)\n",
        "  \n",
        "# This returns an array of r and theta values\n",
        "lines = cv2.HoughLines(edges,1,np.pi/180, 200)\n",
        "  \n",
        "# The below for loop runs till r and theta values \n",
        "# are in the range of the 2d array\n",
        "for r,theta in lines[0]:\n",
        "      \n",
        "    # Stores the value of cos(theta) in a\n",
        "    a = np.cos(theta)\n",
        "  \n",
        "    # Stores the value of sin(theta) in b\n",
        "    b = np.sin(theta)\n",
        "      \n",
        "    # x0 stores the value rcos(theta)\n",
        "    x0 = a*r\n",
        "      \n",
        "    # y0 stores the value rsin(theta)\n",
        "    y0 = b*r\n",
        "      \n",
        "    # x1 stores the rounded off value of (rcos(theta)-1000sin(theta))\n",
        "    x1 = int(x0 + 1000*(-b))\n",
        "      \n",
        "    # y1 stores the rounded off value of (rsin(theta)+1000cos(theta))\n",
        "    y1 = int(y0 + 1000*(a))\n",
        "  \n",
        "    # x2 stores the rounded off value of (rcos(theta)+1000sin(theta))\n",
        "    x2 = int(x0 - 1000*(-b))\n",
        "      \n",
        "    # y2 stores the rounded off value of (rsin(theta)-1000cos(theta))\n",
        "    y2 = int(y0 - 1000*(a))\n",
        "      \n",
        "    # cv2.line draws a line in img from the point(x1,y1) to (x2,y2).\n",
        "    # (0,0,255) denotes the colour of the line to be \n",
        "    #drawn. In this case, it is red. \n",
        "    cv2.line(img,(x1,y1), (x2,y2), (0,0,255),2)\n",
        "      \n",
        "# All the changes made in the input image are finally\n",
        "# written on a new image houghlines.jpg\n",
        "cv2.imwrite('linesDetected.jpg', img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JIVEeTdJfEFO"
      },
      "source": [
        "CIRCLE DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nhS_P3JFfLDi"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import cv2.cv as cv\n",
        "\n",
        "image = cv2.imread('images/bottlecaps.jpg')\n",
        "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "blur = cv2.medianBlur(gray, 5)\n",
        "circles = cv2.HoughCircles(blur, cv.CV_HOUGH_GRADIENT, 1.5, 10)\n",
        "\n",
        "for i in circles[0,:]:\n",
        "       # draw the outer circle\n",
        "       cv2.circle(image,(i[0], i[1]), i[2], (255, 0, 0), 2)\n",
        "      \n",
        "       # draw the center of the circle\n",
        "       cv2.circle(image, (i[0], i[1]), 2, (0, 255, 0), 5)\n",
        "\n",
        "cv2.imshow('detected circles', image)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyulHMeVfMYe"
      },
      "source": [
        "\n",
        "BLOB DETECTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3PFbY6F0fP7W"
      },
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "#read image\n",
        "image = cv2.imread('images/sunflowers.jpg', cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "#set up detector with default parameters\n",
        "detector = cv2.SimpleBlobDetector()\n",
        "\n",
        "#detect blobs\n",
        "keypoints = detector.detect(image)\n",
        "\n",
        "#draw detected blobs as red circles\n",
        "# cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS ensures size of circle of blob\n",
        "blank = np.zeroes((1,1))\n",
        "blobs = cv2.drawKeyPoints(image, keypoints, blank, (0, 255, 0), cv2.DRAW_MATCHES_FLAGS_DEFAULT)\n",
        "\n",
        "#show keypoints\n",
        "cv2.imshow('blobs', blobs)\n",
        "cv2.waitkey(0)\n",
        "cv2,clearAllWindows()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}